{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from model import SpeechRecognition\n",
    "from dataLoader import Data, collate_fn_padd\n",
    "device = \"cuda:0\"\n",
    "\n",
    "batchSize = 64\n",
    "numWorkers = 8\n",
    "def train_dataloader():\n",
    "    d_params = Data.parameters\n",
    "    train_dataset = Data(json_path=\"data/train.json\", **d_params)\n",
    "    return DataLoader(dataset=train_dataset,\n",
    "                        batch_size=batchSize,\n",
    "                        num_workers=numWorkers,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=collate_fn_padd)\n",
    "def valid_dataloader():\n",
    "    d_params = Data.parameters\n",
    "    valid_dataset = Data(json_path=\"data/test.json\", **d_params)\n",
    "    return DataLoader(dataset=valid_dataset,\n",
    "                        batch_size=batchSize,\n",
    "                        num_workers=numWorkers,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=collate_fn_padd)\n",
    "\n",
    "trainloader = train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srmodel = SpeechRecognition()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(srmodel.parameters(), 1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min',\n",
    "    factor=0.50, patience=6)\n",
    "criterion = nn.CTCLoss(blank=28, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, hidden):\n",
    "        return self.model(x, hidden)\n",
    "\n",
    "def step(model, batch):\n",
    "    spectrograms, labels, input_lengths, label_lengths = batch \n",
    "    bs = spectrograms.shape[0]\n",
    "    hidden = model._init_hidden(bs)\n",
    "    hn, c0 = hidden[0].to(device), hidden[1].to(device)\n",
    "    output, _ = model(spectrograms, hidden)\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    return loss\n",
    "\n",
    "def training_step(batch):\n",
    "    loss = step(batch)\n",
    "    return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        loss = step(srmodel, data)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(len(losses))),losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepFlame",
   "language": "python",
   "name": "deepflame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
